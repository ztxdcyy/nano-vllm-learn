- [ ] æŠŠflash-attnæ¢æˆtorch.sdpaï¼Œä¸ç„¶åˆè¦ç¼–è¯‘flashattnå¤ªéº»çƒ¦äº†è·‘ä¸èµ·æ¥
- [ ] å†™ä¸€ä¸ªrequirements.txtæ–¹ä¾¿pipç¯å¢ƒé…ç½®
- [ ] kvcache é©±é€ç­–ç•¥LRU

# æ•°æ®æµå‘

```
graph TD
    A[ç”¨æˆ·è¯·æ±‚] --> B[LLMEngine.generate]
    B --> C[add_request]
    C --> D[Sequenceå¯¹è±¡åˆ›å»º]
    D --> E[Schedulerç­‰å¾…é˜Ÿåˆ—]
    
    F[ä¸»å¾ªç¯] --> G[stepæ–¹æ³•]
    G --> H[Schedulerè°ƒåº¦]
    H --> I[ModelRunneræ¨ç†]
    I --> J[åå¤„ç†æ›´æ–°]
    J --> K[è¿›åº¦æ¡æ›´æ–°]
```

1. example.py åˆå§‹åŒ–llm
    1.1 å®šä¹‰ä¸€ä¸ªæ¨ç†å¼•æ“ç±» LLM -> LLMEngine
    1.2 ç»„ç»‡sampling_paramså’Œprompt beam searchï¼ˆé€‰å–ç»¼åˆæ¦‚ç‡æœ€é«˜çš„topkåºåˆ—ï¼‰ã€params = BeamSearchParams(beam_width=5, max_tokens=50)ã€‘
    1.3 ç”Ÿæˆæ¨¡å‹ç»“æœï¼šè°ƒç”¨llm_engineçš„generateæ–¹æ³•
2. llmæ¥è‡ªäºllm_engineã€‚LLMä½œä¸ºllm_engineçš„å°è£…ï¼Œéšè—äº†åº•å±‚å®ç°ï¼ŒåŒæ—¶åç»­åŠ åŠŸèƒ½ä¹Ÿæ›´æ–¹ä¾¿ã€‚å¯¹äºvllmï¼Œå®ç°äº†ä¸€äº›ç‰¹æ€§ï¼štensor_parallel_sizeã€quantizationã€gpu_memory_utilizationï¼ˆåŒ…å«äº†weightã€activationã€kvcacheï¼Œæ˜¾å­˜åˆ©ç”¨ç‡é«˜çš„æ—¶å€™å¯ä»¥æ”¾æ›´å¤škvcacheä»è€Œæé«˜æ¨¡å‹ååï¼Œä½†æ˜¯å¯èƒ½é€ æˆOOMï¼‰
3. llm_engine æ¨ç†å¼•æ“
è®¾è®¡æ¨ç†æ¡†æ¶çš„æ—¶å€™éœ€è¦è€ƒè™‘æš´éœ²ç»™ç”¨æˆ·çš„èƒ½åŠ›ï¼ˆæ¥å£ï¼‰
![æ¨ç†æ¡†æ¶](img/image1.png)

# LLMEngine
LLMEngine å¯ä»¥ç®€å•åˆ†åˆ«ä¸‰ä¸ªéƒ¨åˆ†ï¼Œschedulerã€block managerã€module_runner
1. scheduelrï¼šç±»ä¼¼äºäº¤è­¦ï¼Œè´Ÿè´£æŒ‡æŒ¥äº¤é€šã€‚è´Ÿè´£ prompt çš„è°ƒåº¦ï¼Œå³é’ˆå¯¹ç”¨æˆ·è¾“å…¥çš„ä¼—å¤š promptsï¼Œé€‰å‡ºå¯ä»¥ä½¿å¾—ååæœ€å¤§çš„é‚£äº› prompts è¿›è¡Œè°ƒåº¦
2. block managerï¼šç®¡ä»“åº“çš„ï¼Œè´Ÿè´£å®‰æ’å†…å­˜ï¼ˆblocksã€slotsï¼‰
3. modle_runnerï¼šå®é™…è´Ÿè´£è®¡ç®—çš„æ¨¡å—ã€‚åœ¨é€šè¿‡ schedule ç¡®å®šå“ªäº› prmopt éœ€è¦è°ƒåº¦æ—¶ï¼ŒçœŸæ­£æ‰§è¡Œä¸€ä¸ªæ¨¡å‹çš„æ¨ç†ã€‚

## ä»£ç æ‹†è§£

### [Generate](nanovllm/engine/llm_engine.py)
1. åˆ›å»ºé‡‡æ ·å‚æ•°spï¼Œæ‰“åŒ…spå’Œpromptåˆ›å»ºåºåˆ—seq
2. æ‰§è¡Œ**æ ¸å¿ƒå¾ªç¯step**ï¼Œè¿”å›outputå’Œnum_tokens
```    
def step(self):
        # è°ƒç”¨è°ƒåº¦å™¨çš„scheduleæ–¹æ³•ï¼Œè¿”å›seqså’Œis_prefill
        seqs, is_prefill = self.scheduler.schedule()
        token_ids = self.model_runner.call("run", seqs, is_prefill)
        self.scheduler.postprocess(seqs, token_ids)
        outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]
        num_tokens = sum(len(seq) for seq in seqs) if is_prefill else -len(seqs)
        return outputs, num_tokens
```
å…·ä½“è°ƒåº¦å™¨è¿‡ç¨‹æ”¾åœ¨åé¢åˆ†æ


### TP
ä½¿ç”¨spawnå¯åŠ¨å¤šè¿›ç¨‹ï¼Œspawné‡‡ç”¨pickleå¤åˆ¶çš„æ–¹å¼ä¼ é€’çˆ¶è¿›ç¨‹çš„å‚æ•°çŠ¶æ€ç­‰ï¼Œæ¯ä¸ªå­è¿›ç¨‹æ˜¯ç‹¬ç«‹çš„ Python è§£é‡Šå™¨ï¼Œä¸ä¼šç»§æ‰¿çˆ¶è¿›ç¨‹çš„ CUDA çŠ¶æ€ï¼Œé¿å… fork å¸¦æ¥çš„ bugã€‚ï¼ˆæ‰€ä»¥å¯åŠ¨TPçš„æ—¶å€™ï¼Œpsçœ‹è¿›ç¨‹çš„æ—¶å€™å¯ä»¥çœ‹åˆ°ä¸€å¤§å †python.multiprocessing.spawnï¼ŒåŸæ¥å¦‚æ­¤ã€‚ï¼‰
[Pythonå¤šè¿›ç¨‹ï¼šspawn and fork](python_multiprocessing.md)

* initè¿™é‡Œï¼Œåˆå§‹åŒ–configã€ä½¿ç”¨torch.multiprocessingå¯åŠ¨å¤šè¿›ç¨‹ï¼Œæ³¨å†Œmodel_runnerï¼ˆåœ¨ä¸»è¿›ç¨‹ä¸Šï¼ŒæŒ‡å®šäº†0ï¼‰`self.model_runner = ModelRunner(config, 0, self.events)`ï¼Œæ³¨å†Œè°ƒåº¦å™¨`self.scheduler = Scheduler(config)`

* add_requestï¼šé¦–å…ˆæ³¨æ„åˆ°è¿™é‡Œpromptå¯ä»¥æ˜¯å­—ç¬¦ä¸²ä¹Ÿå¯ä»¥æ˜¯list[int]ï¼Œå‰è€…å¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯åœ¨example.pyé‡Œçš„`    prompts = [
        "introduce yourself",
        "list all prime numbers within 100",
    ]`ï¼Œå½“ä¼ å…¥å­—ç¬¦ä¸²æ—¶å€™ï¼Œéœ€è¦é€šè¿‡tokenizeråšembeddingå¾—åˆ°token ID åˆ—è¡¨ã€‚åè€…å°±æ˜¯å½“ä½ ä¼ å…¥ä¸€ä¸ªtoken ID åˆ—è¡¨çš„æ—¶å€™ï¼Œç›´æ¥ä½¿ç”¨è¿™ä¸ªåˆ—è¡¨ã€‚**ã€ä½†æ˜¯è¿™é‡Œæ¯”è¾ƒç–‘é—®ğŸ¤”ï¼Œå•¥æ—¶å€™ä¼šç›´æ¥ä¼ å…¥è¿™ä¸ªåˆ—è¡¨å‘¢ï¼Ÿã€‘**

### [Sequence](nanovllm/engine/sequence.py)

1. çŠ¶æ€æœºï¼šWAITING/RUNNING/FINISHED
2. block_tableï¼šç®¡ç†kvcache
3. è£…é¥°å™¨ä½œç”¨ï¼š @propertyï¼šå°†æ–¹æ³•è½¬æ¢ä¸ºåªè¯»å±æ€§ï¼Œè°ƒç”¨æ—¶ä¸éœ€è¦åŠ æ‹¬å·`outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]
`æ¯”å¦‚è¿™é‡Œçš„`seq.is_finished`å°±æ²¡æœ‰åŠ æ‹¬å·ï¼ˆæœ¬èº«æ— å‚æ•°å‡½æ•°å¾—å¼„ä¸€ä¸ªç©ºç€çš„æ‹¬å·ï¼Œç°åœ¨ç”¨äº†è£…é¥°å™¨å°±ä¸ç”¨äº†ï¼‰
4. æ–¹æ³•ï¼š
    1. append_tokenï¼šç”¨äºæµå¼ç”Ÿæˆçš„æ—¶å€™ï¼Œå°†tokenidè¿½åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œæ›´æ–°æœ€åä¸€ä¸ªtokenè¿™ä¸ªå±æ€§ï¼Œä»¥åŠæ›´æ–°num_tokensã€‚åœ¨schdulerä¸­è°ƒç”¨`seq.append_token(token_id)`
    2. __getstate__çœ‹ä¸æ‡‚__setstate__çœ‹ä¸æ‡‚
    3. `if self.num_completion_tokens == 0:`åˆ¤è¯»å½“å‰seqåœ¨prefillè¿˜æ˜¯åœ¨deocdeé˜¶æ®µã€‚prefillçš„è¯å°±ä¼ token_idsåˆ—è¡¨ï¼Œéœ€è¦è®¡ç®—å®Œæ•´çš„promptï¼Œdecodeçš„è¯å°±ä¼ æœ€åä¸€ä¸ªtokenï¼Œå‰é¢çš„promptå·²ç»ç®—å‡ºæ¥kvcacheäº†å°±ä¸ç”¨å†ç®—äº†ã€‚

```
# Prefillé˜¶æ®µç¤ºä¾‹
prompt = "ä¸­å›½çš„é¦–éƒ½æ˜¯"
sequence = Sequence([ä¸­, å›½, çš„, é¦–, éƒ½, æ˜¯])
# __getstate__ ä¼ è¾“: [ä¸­, å›½, çš„, é¦–, éƒ½, æ˜¯] (å®Œæ•´åºåˆ—)

# Decodeé˜¶æ®µç¤ºä¾‹
sequence.append_token(åŒ—)  # ç”Ÿæˆç¬¬ä¸€ä¸ªtoken
sequence.append_token(äº¬)  # ç”Ÿæˆç¬¬äºŒä¸ªtoken
# __getstate__ ä¼ è¾“: äº¬ (ä»…æœ€åä¸€ä¸ªtoken)
```


# Scheduler

(nanovllm/engine/scheduler.py)

## åˆ†æ

* prefill 
    * å–waiting[0]ï¼Œè‹¥èƒ½è°ƒåº¦ï¼Œåˆ™popleftå¹¶åŠ å…¥running
    * num_batched_tokensï¼šå½“å‰batchä¸­æ‰€æœ‰éœ€è¦è®¡ç®—çš„tokenæ€»æ•°ï¼ˆseq0éœ€è¦è®¡ç®—8kä¸ªï¼Œseq1éœ€è¦è®¡ç®—9kä¸ªï¼Œæ­¤æ—¶seq1ä¼šè¢«æ‹’ç»ï¼‰
    * self.block_manager.can_allocate(seq)è¿”å›falseï¼Œæ­¤æ—¶ä¹Ÿä¼šæ‹’ç»è°ƒåº¦

* decode
    * å–running[0]ï¼Œå‡†å¤‡ä¸ºæ¯ä¸ªseqè®¡ç®—ä¸€ä¸ªtoken
    * can_appendï¼š`return len(self.free_block_ids) >= (len(seq) % self.block_size == 1)`
        * len(seq) = 255 æ­¤æ—¶å¯¹256ï¼ˆself.block_sizeï¼‰å–ä½™æ•°ï¼Œå¾—åˆ°255ï¼Œå¹¶ä¸æ˜¯1ï¼Œåˆ¤åˆ«å¼è¿”å›0ã€‚åˆ¤æ–­free_block_idsæ˜¯å¦éœ€è¦å¤§äºç­‰äº0ï¼Œæ­¤æ—¶è‚¯å®šæ˜¯Trueçš„ã€‚
        * len(seq) = 257 257%256=1 åˆ¤åˆ«å¼è¿”å›Trueä¹Ÿå°±æ˜¯1ï¼Œåˆ¤æ–­free_block_idsæ˜¯å¦éœ€è¦å¤§äºç­‰äº1ï¼Œä¹Ÿå°±æ„å‘³ç€ç”³è¯·ä¸€ä¸ªæ–°çš„blockã€‚  


preempt

## ä»£ç æ‹†è§£



# Attention

## Triton å…¥é—¨æ¦‚å¿µ

ç¨‹åºå¹¶è¡Œæ¨¡å‹
Triton ç±»ä¼¼ CUDAï¼Œä¼šæŠŠ kernel åˆ†æˆå¾ˆå¤š program instances æ‰§è¡Œï¼Œæ¯ä¸ª instance å¤„ç†ä¸€éƒ¨åˆ†æ•°æ®ã€‚

tl.program_id(0) å°±æ˜¯ã€Œå½“å‰ instance çš„ idã€ï¼Œä¸€èˆ¬ç”¨æ¥å¯¹åº” batch ä¸­çš„ç¬¬å‡ ä¸ªæ ·æœ¬ã€‚

tl.arange(0, D) ç”Ÿæˆä¸€ä¸ª [0, 1, 2, ..., D-1] çš„å‘é‡ï¼Œå¸¸ç”¨äºä¸€æ¬¡æ€§åŠ è½½/å­˜å‚¨ä¸€æ®µè¿ç»­å†…å­˜ã€‚

å†…å­˜æ“ä½œ

tl.load(ptr + offsets)ï¼šä»æ˜¾å­˜è¯»å–ä¸€æ®µæ•°æ®ã€‚offsets é€šå¸¸æ˜¯ vectorï¼Œå¯ä»¥ä¸€æ¬¡è¯»å¤šä¸ªã€‚

tl.store(ptr + offsets, data)ï¼šå¾€æ˜¾å­˜å†™ä¸€æ®µæ•°æ®ã€‚

Triton é»˜è®¤ä¼šå¹¶è¡ŒåŒ–è¿™äº›å‘é‡æ“ä½œï¼Œæ¯ä¸ªçº¿ç¨‹å—å†…éƒ¨èƒ½åŒæ—¶å¤„ç†ä¸€æ‰¹å…ƒç´ ã€‚

kernel è°ƒç”¨æ–¹å¼

store_kvcache_kernel[(N,)](...) â†’ è¿™é‡Œ (N,) è¡¨ç¤º å¯åŠ¨ N ä¸ª program instanceï¼Œæ¯ä¸ª instance è´Ÿè´£å¤„ç†ä¸€ä¸ªæ ·æœ¬çš„ key/valueã€‚

ç›¸æ¯” CUDAï¼ŒTriton æŠŠã€Œblock/grid/threadã€æŠ½è±¡æ‰äº†ï¼Œç”¨æ›´é«˜å±‚çš„ã€Œprogramã€æ¥è¡¨ç¤ºã€‚

## ä¸ºä»€ä¹ˆé€‰æ‹©tritonåšå†…å­˜æ§åˆ¶ï¼Œè€Œä¸æ˜¯ä¸€è‚¡è„‘äº¤ç»™pytorchï¼Ÿ

é€»è¾‘æ˜¯å¯¹çš„ï¼Œä½† PyTorch å†…éƒ¨ä¼šèµ°ä¸€å¤§å †æµç¨‹ï¼š

è°ƒç”¨ dispatcher â†’ æ‰¾åˆ°å¯¹åº”çš„ CUDA kernel â†’ å¯åŠ¨ kernel â†’ æŠŠæ•°æ® copyã€‚

æ¯ä¸€æ¬¡å†™å…¥éƒ½æ˜¯ä¸€æ¬¡ã€Œå…¨å±€è°ƒåº¦ + kernel å¯åŠ¨ã€ã€‚

å¦‚æœä½ çš„æ“ä½œå¾ˆç»†ç²’åº¦ï¼ˆæ¯”å¦‚ KVCache æ¯ä¸ª step åªå†™å¾ˆå°‘çš„æ•°æ®ï¼‰ï¼Œè¿™ç§é¢å¤–å¼€é”€éå¸¸æ˜æ˜¾ã€‚

æ‰€ä»¥åœ¨å¤§æ¨¡å‹æ¨ç†ä¸­ï¼ˆå°¤å…¶æ˜¯ prefill ä¹‹åçš„ decode é˜¶æ®µï¼‰ï¼ŒPyTorch çº§åˆ«çš„ op ä¼šæˆä¸º ç“¶é¢ˆã€‚

KVCache çš„ å†™å…¥é‡å¾ˆå°ä½†å¾ˆé¢‘ç¹ï¼ˆæ¯ç”Ÿæˆä¸€ä¸ª tokenï¼Œå°±è¦å†™ä¸€æ¬¡ï¼‰ã€‚

å°é‡é¢‘ç¹å†™ï¼Œå¦‚æœç”¨ PyTorchï¼Œè°ƒåº¦å¼€é”€ > çœŸæ­£è®¡ç®—/å†…å­˜æ“ä½œå¼€é”€ã€‚

Triton å¯ä»¥æŠŠè¿™äº›ã€Œå°è€Œå¤šã€çš„å†™åˆå¹¶æˆä¸€ä¸ª kernelï¼Œé«˜æ•ˆæ¬è¿ã€‚

## store_kvcache_kernel
â€œè¯»å…¥å½“å‰ batch çš„æ–° token çš„ key/value å‘é‡ï¼ˆä» key/value tensorï¼‰ï¼Œæ ¹æ® slot_mapping æ‰¾åˆ°ç›®æ ‡ä½ç½®ï¼ŒæŠŠå®ƒä»¬å†™è¿› KVCacheã€‚â€




# Linear

## QKVParallelLinearï¼ˆColumnParallelLinearï¼‰

æ ¹æ®ç±»å‹è®¡ç®— åˆ‡åˆ†çš„åç§»é‡ (shard_offset) å’Œ å¤§å° (shard_size)ï¼š

Qï¼šåç§»é‡ä¸º 0ï¼Œå› ä¸ºå®ƒæ˜¯ç¬¬ä¸€ä¸ªã€‚

Kï¼šåç§»é‡åœ¨ Q ä¹‹åï¼Œå³ self.num_heads * self.head_sizeã€‚

Vï¼šåç§»é‡åœ¨ Q å’Œ K ä¹‹åï¼Œå³ self.num_heads * self.head_size + self.num_kv_heads * self.head_sizeã€‚

```
param_data = param_data.narrow(self.tp_dim, shard_offset, shard_size)
loaded_weight = loaded_weight.chunk(self.tp_size, self.tp_dim)[self.tp_rank]
param_data.copy_(loaded_weight)
```

å®šä½ç›®æ ‡ï¼ˆnarrowï¼‰: â€œæˆ‘çš„ GPU ä¸Šæœ‰ä¸€ä¸ªå¾ˆå¤§çš„å†…å­˜åŒºåŸŸï¼Œæˆ‘è¦åœ¨è¿™å—åŒºåŸŸçš„ç‰¹å®šä½ç½®ï¼Œä¸ºæˆ‘çš„ K çŸ©é˜µæ‰¾åˆ°ä¸€ä¸ªå­˜å‚¨ç©ºé—´ã€‚â€

è·å–æ•°æ®ï¼ˆchunkï¼‰: â€œç£ç›˜ä¸Šæœ‰ä¸€ä¸ªå®Œæ•´çš„ K çŸ©é˜µæ–‡ä»¶ï¼Œæˆ‘è¦ä»ä¸­å–å‡ºä¸“é—¨åˆ†é…ç»™æˆ‘è¿™å° GPU çš„é‚£ä¸€å°éƒ¨åˆ†æ•°æ®ã€‚â€

å†™å…¥æ•°æ®ï¼ˆcopy_ï¼‰: â€œç°åœ¨æˆ‘æœ‰äº†æˆ‘çš„å­˜å‚¨ç©ºé—´ï¼Œä¹Ÿæœ‰äº†æˆ‘çš„æ•°æ®ï¼Œæˆ‘æŠŠå®ƒä¿©ç²¾ç¡®åœ°å¯¹æ¥èµ·æ¥ï¼ŒæŠŠæ•°æ®å†™å…¥åˆ°å¯¹åº”çš„å†…å­˜ä½ç½®é‡Œå»ã€‚â€

param_data.narrow(...)ï¼šè¿™ä¸ªæ“ä½œç”¨äºåœ¨å½“å‰ GPU çš„å‚æ•°å¼ é‡ä¸­ï¼Œæ‰¾åˆ°å¯¹åº”äºå½“å‰ shard çš„é‚£ä¸€éƒ¨åˆ†ã€‚

loaded_weight.chunk(...)ï¼šå°†åŠ è½½çš„æƒé‡ loaded_weight æ²¿ç€å¼ é‡å¹¶è¡Œç»´åº¦ (self.tp_dim) åˆ‡åˆ†ï¼Œå¹¶åªå–å½“å‰ GPU å¯¹åº”çš„éƒ¨åˆ† ([self.tp_rank])ã€‚

param_data.copy_(loaded_weight)ï¼šæœ€åï¼Œå°†åˆ‡åˆ†åçš„æƒé‡ loaded_weight å¤åˆ¶åˆ°æœ¬åœ°çš„å‚æ•°å¼ é‡ä¸­ï¼Œå®Œæˆäº†æƒé‡çš„åŠ è½½ã€‚
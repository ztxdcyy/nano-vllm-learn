- [ ] æŠŠflash-attnæ¢æˆtorch.sdpaï¼Œä¸ç„¶åˆè¦ç¼–è¯‘flashattnå¤ªéº»çƒ¦äº†è·‘ä¸èµ·æ¥
- [ ] å†™ä¸€ä¸ªrequirements.txtæ–¹ä¾¿pipç¯å¢ƒé…ç½®
- [ ] kvcache é©±é€ç­–ç•¥LRU

# æ•°æ®æµå‘

```
graph TD
    A[ç”¨æˆ·è¯·æ±‚] --> B[LLMEngine.generate]
    B --> C[add_request]
    C --> D[Sequenceå¯¹è±¡åˆ›å»º]
    D --> E[Schedulerç­‰å¾…é˜Ÿåˆ—]
    
    F[ä¸»å¾ªç¯] --> G[stepæ–¹æ³•]
    G --> H[Schedulerè°ƒåº¦]
    H --> I[ModelRunneræ¨ç†]
    I --> J[åå¤„ç†æ›´æ–°]
    J --> K[è¿›åº¦æ¡æ›´æ–°]
```

1. example.py åˆå§‹åŒ–llm
    1.1 å®šä¹‰ä¸€ä¸ªæ¨ç†å¼•æ“ç±» LLM -> LLMEngine
    1.2 ç»„ç»‡sampling_paramså’Œprompt beam searchï¼ˆé€‰å–ç»¼åˆæ¦‚ç‡æœ€é«˜çš„topkåºåˆ—ï¼‰ã€params = BeamSearchParams(beam_width=5, max_tokens=50)ã€‘
    1.3 ç”Ÿæˆæ¨¡å‹ç»“æœï¼šè°ƒç”¨llm_engineçš„generateæ–¹æ³•
2. llmæ¥è‡ªäºllm_engineã€‚LLMä½œä¸ºllm_engineçš„å°è£…ï¼Œéšè—äº†åº•å±‚å®ç°ï¼ŒåŒæ—¶åç»­åŠ åŠŸèƒ½ä¹Ÿæ›´æ–¹ä¾¿ã€‚å¯¹äºvllmï¼Œå®ç°äº†ä¸€äº›ç‰¹æ€§ï¼štensor_parallel_sizeã€quantizationã€gpu_memory_utilizationï¼ˆåŒ…å«äº†weightã€activationã€kvcacheï¼Œæ˜¾å­˜åˆ©ç”¨ç‡é«˜çš„æ—¶å€™å¯ä»¥æ”¾æ›´å¤škvcacheä»è€Œæé«˜æ¨¡å‹ååï¼Œä½†æ˜¯å¯èƒ½é€ æˆOOMï¼‰
3. llm_engine æ¨ç†å¼•æ“
è®¾è®¡æ¨ç†æ¡†æ¶çš„æ—¶å€™éœ€è¦è€ƒè™‘æš´éœ²ç»™ç”¨æˆ·çš„èƒ½åŠ›ï¼ˆæ¥å£ï¼‰
![æ¨ç†æ¡†æ¶](img/image1.png)

# LLMEngine
LLMEngine å¯ä»¥ç®€å•åˆ†åˆ«ä¸‰ä¸ªéƒ¨åˆ†ï¼Œschedulerã€block managerã€module_runner
1. scheduelrï¼šç±»ä¼¼äºäº¤è­¦ï¼Œè´Ÿè´£æŒ‡æŒ¥äº¤é€šã€‚è´Ÿè´£ prompt çš„è°ƒåº¦ï¼Œå³é’ˆå¯¹ç”¨æˆ·è¾“å…¥çš„ä¼—å¤š promptsï¼Œé€‰å‡ºå¯ä»¥ä½¿å¾—ååæœ€å¤§çš„é‚£äº› prompts è¿›è¡Œè°ƒåº¦
2. block managerï¼šç®¡ä»“åº“çš„ï¼Œè´Ÿè´£å®‰æ’å†…å­˜ï¼ˆblocksã€slotsï¼‰
3. modle_runnerï¼šå®é™…è´Ÿè´£è®¡ç®—çš„æ¨¡å—ã€‚åœ¨é€šè¿‡ schedule ç¡®å®šå“ªäº› prmopt éœ€è¦è°ƒåº¦æ—¶ï¼ŒçœŸæ­£æ‰§è¡Œä¸€ä¸ªæ¨¡å‹çš„æ¨ç†ã€‚

## ä»£ç æ‹†è§£

### [Generate](nanovllm/engine/llm_engine.py)
1. åˆ›å»ºé‡‡æ ·å‚æ•°spï¼Œæ‰“åŒ…spå’Œpromptåˆ›å»ºåºåˆ—seq
2. æ‰§è¡Œ**æ ¸å¿ƒå¾ªç¯step**ï¼Œè¿”å›outputå’Œnum_tokens
```    
def step(self):
        # è°ƒç”¨è°ƒåº¦å™¨çš„scheduleæ–¹æ³•ï¼Œè¿”å›seqså’Œis_prefill
        seqs, is_prefill = self.scheduler.schedule()
        token_ids = self.model_runner.call("run", seqs, is_prefill)
        self.scheduler.postprocess(seqs, token_ids)
        outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]
        num_tokens = sum(len(seq) for seq in seqs) if is_prefill else -len(seqs)
        return outputs, num_tokens
```
å…·ä½“è°ƒåº¦å™¨è¿‡ç¨‹æ”¾åœ¨åé¢åˆ†æ


### TP
ä½¿ç”¨spawnå¯åŠ¨å¤šè¿›ç¨‹ï¼Œspawné‡‡ç”¨pickleå¤åˆ¶çš„æ–¹å¼ä¼ é€’çˆ¶è¿›ç¨‹çš„å‚æ•°çŠ¶æ€ç­‰ï¼Œæ¯ä¸ªå­è¿›ç¨‹æ˜¯ç‹¬ç«‹çš„ Python è§£é‡Šå™¨ï¼Œä¸ä¼šç»§æ‰¿çˆ¶è¿›ç¨‹çš„ CUDA çŠ¶æ€ï¼Œé¿å… fork å¸¦æ¥çš„ bugã€‚ï¼ˆæ‰€ä»¥å¯åŠ¨TPçš„æ—¶å€™ï¼Œpsçœ‹è¿›ç¨‹çš„æ—¶å€™å¯ä»¥çœ‹åˆ°ä¸€å¤§å †python.multiprocessing.spawnï¼ŒåŸæ¥å¦‚æ­¤ã€‚ï¼‰
[Pythonå¤šè¿›ç¨‹ï¼šspawn and fork](python_multiprocessing.md)

* initè¿™é‡Œï¼Œåˆå§‹åŒ–configã€ä½¿ç”¨torch.multiprocessingå¯åŠ¨å¤šè¿›ç¨‹ï¼Œæ³¨å†Œmodel_runnerï¼ˆåœ¨ä¸»è¿›ç¨‹ä¸Šï¼ŒæŒ‡å®šäº†0ï¼‰`self.model_runner = ModelRunner(config, 0, self.events)`ï¼Œæ³¨å†Œè°ƒåº¦å™¨`self.scheduler = Scheduler(config)`

* add_requestï¼šé¦–å…ˆæ³¨æ„åˆ°è¿™é‡Œpromptå¯ä»¥æ˜¯å­—ç¬¦ä¸²ä¹Ÿå¯ä»¥æ˜¯list[int]ï¼Œå‰è€…å¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯åœ¨example.pyé‡Œçš„`    prompts = [
        "introduce yourself",
        "list all prime numbers within 100",
    ]`ï¼Œå½“ä¼ å…¥å­—ç¬¦ä¸²æ—¶å€™ï¼Œéœ€è¦é€šè¿‡tokenizeråšembeddingå¾—åˆ°token ID åˆ—è¡¨ã€‚åè€…å°±æ˜¯å½“ä½ ä¼ å…¥ä¸€ä¸ªtoken ID åˆ—è¡¨çš„æ—¶å€™ï¼Œç›´æ¥ä½¿ç”¨è¿™ä¸ªåˆ—è¡¨ã€‚**ã€ä½†æ˜¯è¿™é‡Œæ¯”è¾ƒç–‘é—®ğŸ¤”ï¼Œå•¥æ—¶å€™ä¼šç›´æ¥ä¼ å…¥è¿™ä¸ªåˆ—è¡¨å‘¢ï¼Ÿã€‘**

### [Sequence](nanovllm/engine/sequence.py)

1. çŠ¶æ€æœºï¼šWAITING/RUNNING/FINISHED
2. block_tableï¼šç®¡ç†kvcache
3. è£…é¥°å™¨ä½œç”¨ï¼š @propertyï¼šå°†æ–¹æ³•è½¬æ¢ä¸ºåªè¯»å±æ€§ï¼Œè°ƒç”¨æ—¶ä¸éœ€è¦åŠ æ‹¬å·`outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]
`æ¯”å¦‚è¿™é‡Œçš„`seq.is_finished`å°±æ²¡æœ‰åŠ æ‹¬å·ï¼ˆæœ¬èº«æ— å‚æ•°å‡½æ•°å¾—å¼„ä¸€ä¸ªç©ºç€çš„æ‹¬å·ï¼Œç°åœ¨ç”¨äº†è£…é¥°å™¨å°±ä¸ç”¨äº†ï¼‰
4. æ–¹æ³•ï¼š
    1. append_tokenï¼šç”¨äºæµå¼ç”Ÿæˆçš„æ—¶å€™ï¼Œå°†tokenidè¿½åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œæ›´æ–°æœ€åä¸€ä¸ªtokenè¿™ä¸ªå±æ€§ï¼Œä»¥åŠæ›´æ–°num_tokensã€‚åœ¨schdulerä¸­è°ƒç”¨`seq.append_token(token_id)`
    2. __getstate__çœ‹ä¸æ‡‚__setstate__çœ‹ä¸æ‡‚
    3. `if self.num_completion_tokens == 0:`åˆ¤è¯»å½“å‰seqåœ¨prefillè¿˜æ˜¯åœ¨deocdeé˜¶æ®µã€‚prefillçš„è¯å°±ä¼ token_idsåˆ—è¡¨ï¼Œéœ€è¦è®¡ç®—å®Œæ•´çš„promptï¼Œdecodeçš„è¯å°±ä¼ æœ€åä¸€ä¸ªtokenï¼Œå‰é¢çš„promptå·²ç»ç®—å‡ºæ¥kvcacheäº†å°±ä¸ç”¨å†ç®—äº†ã€‚

```
# Prefillé˜¶æ®µç¤ºä¾‹
prompt = "ä¸­å›½çš„é¦–éƒ½æ˜¯"
sequence = Sequence([ä¸­, å›½, çš„, é¦–, éƒ½, æ˜¯])
# __getstate__ ä¼ è¾“: [ä¸­, å›½, çš„, é¦–, éƒ½, æ˜¯] (å®Œæ•´åºåˆ—)

# Decodeé˜¶æ®µç¤ºä¾‹
sequence.append_token(åŒ—)  # ç”Ÿæˆç¬¬ä¸€ä¸ªtoken
sequence.append_token(äº¬)  # ç”Ÿæˆç¬¬äºŒä¸ªtoken
# __getstate__ ä¼ è¾“: äº¬ (ä»…æœ€åä¸€ä¸ªtoken)
```


# [Scheduler](nanovllm/engine/scheduler.py)

## åˆ†æ

## ä»£ç æ‹†è§£




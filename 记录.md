一开始自己手动写了 GQA 中重复 attention head的逻辑
但是仔细看了一下示例代码，看到其实pytorch 会处理重复 head

https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html

这个人的版本也跑不起来：

https://github.com/halo-geng/nano-vllm/tree/toy_without_flash_attn

https://github.com/halo-geng/nano-vllm/commit/176d1efd51e92c1c23f698d0472a01278ab29327

```
def flash_attn_with_kvcache(
    q: torch.Tensor,
    k_cache: torch.Tensor,
    v_cache: torch.Tensor,
    cache_seqlens: torch.Tensor,
    block_table: Optional[torch.Tensor] = None,
    softmax_scale: float = 1.0,
    causal: bool = True,
) -> torch.Tensor:

    # 使用列表推导式，一行代码transpose三个变量
    q_sdpa, k_sdpa, v_sdpa = [tensor.transpose(1, 2) for tensor in [q, k_cache, v_cache]]

    # 使用 SDPA 计算注意力
    with sdpa_kernel(backends=[SDPBackend.MATH]):
        output = F.scaled_dot_product_attention(
            q_sdpa, k_sdpa, v_sdpa,
            attn_mask=None,
            dropout_p=0.0,
            is_causal=causal,
            enable_gqa=True,
            scale=softmax_scale
        )
    
    output = output.transpose(1, 2)
    return output
```

这里 scale 也和 API 不一致：

scale (optional python:float, keyword-only) – Scaling factor applied prior to softmax. If None, the default value is set to $\frac{1}{\sqrt{E}}$

我们改成 None 就可以

试图做对比 DEBUG：

```
        if context.is_prefill:
            if context.block_tables is not None:    # prefix cache
                k, v = k_cache, v_cache
            
            # 在这里打印 fa prefill 的输入，用 pickle 保存对象，之后在 sdpa 里读 pickle，转序列化 验证 prefill 接口正确性
            o = flash_attn_varlen_func(q, k, v,
                                       max_seqlen_q=context.max_seqlen_q, cu_seqlens_q=context.cu_seqlens_q,
                                       max_seqlen_k=context.max_seqlen_k, cu_seqlens_k=context.cu_seqlens_k,
                                       softmax_scale=self.scale, causal=True, block_table=context.block_tables)
            # print(o)
        else:    # decode
            # 通过context.context_lens来区分不同序列
            o = flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache,
                                        cache_seqlens=context.context_lens, block_table=context.block_tables, 
                                        softmax_scale=self.scale, causal=True)
        o = o.view(-1, self.num_heads * self.head_dim)
```

FA 需要 softmax scale，而 sdpa 不需要，传入 None 就可以使用默认值

---


[karpathy的项目](https://github.com/karpathy/nanoGPT)也没有设计 kvcache 的管理，我决定去参考 HF transformers的实现：
[HF](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py)



# kvcache: slot mapping & block table

slot_mapping ：指出该 token 对应的 kvcache 在内存中的全局 idx。（slot）在 attention kernel 中，我们根据 slot_mapping 去对应 load and store 该位置上的 kvcache 到物理地址上。

triton kernel：将计算好的新 token 的 kvcache 写入对应 slot 位置

block table：记录保存可复用的完整 kvcache block 的表

kvcache shape [num_blocks, block_size, num_kv_heads, head_dim]

```
print(f"[DEBUG] BLOCK TABLE: {block_tables} \n[DEBUG] SLOT MAPPING: {slot_mapping}")
```

```
[DEBUG PREFILL] BLOCK TABLE: None 
[DEBUG PREFILL] SLOT MAPPING: tensor([], device='cuda:0', dtype=torch.int32)
[DEBUG PREFILL] BLOCK TABLE: None 
[DEBUG PREFILL] SLOT MAPPING: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,
        267, 268, 269, 270], device='cuda:0', dtype=torch.int32)
[DEBUG DECODE] BLOCK TABLE: tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32) 
[DEBUG DEOCDE] SLOT MAPPING: tensor([ 17, 271], device='cuda:0', dtype=torch.int32)
```


学习 [flash infer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html) 后端实现：
【PASS】


## Padding seq for batch attn（sdpa）

sdpa 在计算 attn 的时候，可以接受 batch seq。那肯定有个问题，每个 seq 长度不一致，那么为了构成 batch，一定会有 padding to maxlen 这个步骤，我想知道我应该 padding 0 还是-inf？ sdpa kernel 好像没有接受这个 padding mask 的地方，他内部会自己处理吗？

看 sdpa kernel 的 doc ，如何计算 batch attn？

attn_mask (optional Tensor) – Attention mask; shape must be broadcastable to the shape of attention weights, which is(N,...,L,S). Two types of masks are supported. A boolean mask where a value of True indicates that the element should take part in attention. A float mask of the same type as query, key, value that is added to the attention score.

attnmask 支持两种，一种是 bool，一种是加性的浮点 mask

is_causal (bool) – If set to true, the attention masking is a lower triangular matrix when the mask is a square matrix. The attention masking has the form of the upper left causal bias due to the alignment (see torch.nn.attention.bias.CausalBias) when the mask is a non-square matrix. An error is thrown if both attn_mask and is_causal are set.

是否是因果？假如是，则 mask 是一个下三角矩阵。同时设置 attnmask 和 iscausal 会报错

## Problem Solved！！
问题就出在kvcache上，我们正确的拼接了kvcache以及创建mask传递给 sdpa kernel 即可！！
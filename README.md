# nano-vllm-learn
# ç‰¹ç‚¹
1. è¯¦ç»†çš„æ³¨é‡Šï¼Œæ–¹ä¾¿å­¦ä¹ 
2. without-flash-attnçš„å®ç°ï¼šæˆ‘è§‰å¾—flash-attnåœ¨æœ€å¼€å§‹å®‰è£…ç¯å¢ƒæ—¶å€™æ—¶å€™éœ€è¦ç¼–è¯‘æ¯”è¾ƒèŠ±æ—¶é—´ï¼Œå°±æƒ³åšä¸€ä¸ªwo-flash-attnçš„ç‰ˆæœ¬ã€‚mainåˆ†æ”¯å·²ç»åˆå¹¶äº†å®Œæ•´å®ç°ï¼Œå¹¶ä¸”æ”¹äº†benchï¼Œä¸‹é¢è¯´ï¼›wo_flash_attnåˆ†æ”¯è®°å½•äº†å¼€å‘è¿‡ç¨‹ã€‚
3. æ”¹äº†ä¸€ä¸ªå°BUGï¼šåŸæ¥çš„block_manageræ²¡æœ‰è€ƒè™‘è¿‡çŸ­prompt=1è§¦å‘assert errorçš„æƒ…å†µï¼Œæ–°å¢äº†ä¸€ä¸ªè¡¥å¿é€»è¾‘ã€‚

## Bugfixï¼šwarmup çŸ­ prompt è§¦å‘ block_manager æ–­è¨€
- ç°è±¡ï¼šè·‘ `bench_my.py` æ—¶ warmup é˜¶æ®µå´©åœ¨ `assert last_block.hash != -1`ï¼ˆ`may_append` çš„ `%block_size==1` åˆ†æ”¯ï¼‰ã€‚
- æ ¹å› ï¼šwarmup prompt å¾ˆçŸ­ï¼ˆé€šå¸¸ 1~å‡  tokenï¼‰ï¼Œ`allocate` ä¸ä¼šç»™æœªæ»¡å—å†™ hashï¼›decode æ—¶ `len%block_size==1` ä¾ç„¶ä¼šè°ƒ `may_append`ï¼ŒåŸæ–­è¨€è¯¯ä»¥ä¸ºä¸Šä¸€å—å¿…é¡»å·²æœ‰ hashã€‚
- æ–¹æ¡ˆï¼šåœ¨è¿›å…¥ `%block_size==1` åˆ†æ”¯æ—¶ï¼Œå¦‚æœä¸Šä¸€å—å°šæœªå†™ hashï¼Œå°±è¡¥å¿è®¡ç®—ï¼ˆåªæœ‰å—æ»¡æ‰å†™ï¼‰ã€‚å¯¹åˆæ³•çš„çŸ­åºåˆ—ä¸å†æ–­è¨€ä¸­æ–­ã€‚

## Triton å®ç°çš„ Flash-Attention V2

FA v1 å’Œ v2 çš„æ ¸å¿ƒåŒºåˆ«ï¼ˆè®ºæ–‡æ€»ç»“ï¼‰ï¼š

1.	å‡å°‘ non-matmul FLOPsï¼ˆä¾‹å¦‚æ”¹å†™è¾“å‡ºæ›´æ–°å½¢å¼ã€åªå­˜ logsumexp ç­‰ï¼‰ ï¿¼
2.	æ²¿ sequence lengthï¼ˆquery ç»´åº¦ï¼‰å¢åŠ å¹¶è¡Œåº¦ï¼Œé•¿åºåˆ—å° batch/å°å¤´æ•°æ—¶ä¹Ÿèƒ½æŠŠ SM åƒæ»¡ 
3.	block å†… warp åˆ†å·¥æ›´åˆç†ï¼Œå‡å°‘ shared-memory è¯»å†™å’ŒåŒæ­¥ï¼ˆæå‡ååï¼‰ ï¿¼

è¿™é‡Œæ˜¯online-softmaxçš„ä¼ªä»£ç å®ç°ï¼Œå¯ä»¥çœ‹åˆ°ï¼š

* å¤–å±‚å›ºå®š Q block â‡’ è¿™ä¸ª block çš„ $Q_iã€running m_i,\ell_i$ã€ä»¥åŠè¾“å‡ºç´¯åŠ å™¨ $O_i$ å¯ä»¥åœ¨ç‰‡ä¸ŠæŒç»­å­˜åœ¨
* å†…å±‚æ‰«æ‰€æœ‰ K/V blocks â‡’ åªéœ€ä¸æ–­æµå¼åŠ è½½ K/V tilesï¼Œæ›´æ–°è¿™ä¸€ä¸ª $O_i$ï¼ˆåœ¨paged-attnä¸­ï¼Œkvæ˜¯åˆ†å—çš„ï¼Œéœ€è¦ä»block-tableä¸­è®¡ç®—å‡ºå®é™…çš„ç‰©ç†ä½ç½®ç„¶åæ‹¿åˆ°ï¼‰
* æœ€ç»ˆæŠŠ $O_i$ å†™å› HBM ä¸€æ¬¡ï¼Œé¿å… FA1 â€œæ¯ä¸ª KV tile éƒ½è¦æŠŠ $O_i$ è¯»å†™å›HBMâ€œ çš„é«˜å»¶è¿Ÿæ“ä½œã€‚

```
# åˆå§‹åŒ–ç´¯ç§¯å™¨
acc = tl.zeros([head_dim], dtype=tl.float32)  # åŠ æƒå’Œ
l_i = 0.0                                       # å½’ä¸€åŒ–å› å­
m_i = -1e10                                     # æœ€å¤§å€¼è·Ÿè¸ª

# å¯¹æ¯ä¸ªchunkå¤„ç†
for chunk_idx in range(max_chunks):
    # è®¡ç®—å½“å‰chunkçš„æ³¨æ„åŠ›åˆ†æ•°
    qk = compute_attention_scores(q, k_chunk)
    
    # åœ¨çº¿softmaxæ›´æ–°
    m_ij = tl.max(qk)
    m_i_new = tl.maximum(m_i, m_ij)
    alpha = tl.exp(m_i - m_i_new)
    
    # é‡æ–°ç¼©æ”¾ç´¯ç§¯å™¨
    acc = acc * alpha
    l_i = l_i * alpha
    
    # åŠ æƒç´¯ç§¯Vå‘é‡
    acc = acc + sum(p * v_chunk)
    l_i = l_i + sum(p)

# å¾ªç¯ç»“æŸåå†åšä¸€æ¬¡å½’ä¸€åŒ–å’Œå†™å›ï¼Œå‡å°‘é mm è®¡ç®—å’Œå†™ HBM çš„æ¬¡æ•°
output = acc / l_i
store(output)
```

å†™äº†ä¸€ä¸ªå°è„šæœ¬ï¼š[`flash_attention_example.py`](flash_attention_example.py)

ï¼ˆæ—§ç‰ˆâ€œSDPA æ›¿ä»£ flash-attnâ€çš„è¸©å‘ç¬”è®°å·²ç§»è‡³æ–‡æœ«â€œå¼€å‘è®°å½•ï¼ˆå­˜æ¡£ï¼‰â€ã€‚ï¼‰

## åˆ é™¤official benchçš„éšæœºæ€§ï¼Œæ·»åŠ å‘½ä»¤è¡Œå‚æ•°`--attn-backend`
æˆ‘ä¿®æ”¹äº†[official benchä»£ç ](bench.py)çš„éšæœºæ€§ï¼ŒåŸæ¥ä»–çš„ä»£ç æ˜¯å®šä¸‹ä¸€ä¸ªmax-input-lenå’Œmax-output-lenï¼Œç„¶å`randint(100, max)`éšæœºå–è¾“å…¥è¾“å‡ºé•¿åº¦ï¼Œæˆ‘è§‰å¾—è¿˜æ˜¯å®šä¸‹æ¥æˆ‘æ¯”è¾ƒå®‰å¿ƒï¼Œå°±æŠŠrandintåˆ æ‰äº†ã€‚

åŒæ—¶æ–°å¢äº†`--attn-backend`çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨æ¥æŒ‡å®šä½¿ç”¨sdpaã€tritonã€flashï¼Œé»˜è®¤æ˜¯flashï¼ˆflash-attentionï¼‰ã€‚

è·‘ä¸‹æ¥ç»“æœï¼š

```
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench.py --attn-backend flash
`torch_dtype` is deprecated! Use `dtype` instead!
Total: 262144tok, Time: 64.33s, Throughput: 4075.22tok/s
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench.py --attn-backend sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Total: 262144tok, Time: 64.34s, Throughput: 4074.61tok/s
```

## è¡¨æ ¼å¯¹æ¯”åŠŸèƒ½ bench_my.py

ä¸ºäº†æ›´ç³»ç»Ÿåœ°æ¯”è¾ƒä¸åŒ attention åç«¯åœ¨ **decode åå**ä¸Šçš„å·®å¼‚ï¼Œæˆ‘å†™äº†ä¸€ä¸ªè¡¨æ ¼ç‰ˆçš„ benchï¼š[`bench_my.py`](bench_my.py)ã€‚

å®ƒçš„ç‰¹ç‚¹ï¼š
- **ä¸€æ¬¡è·‘ä¸‰ç§åç«¯**ï¼š`flash` / `triton` / `sdpa.math`ï¼ˆç”¨ `--attn-backend` ä¼  listï¼‰ã€‚
- **ä¸€æ¬¡è·‘å¤šç»„ batch size + å¤šç»„é•¿åº¦**ï¼šç”¨ `--batch-sizes` å’Œ `--input-lens` ä¼  listã€‚
- **åªç»Ÿè®¡ output token çš„ååï¼ˆtok/sï¼‰**ï¼šå¿½ç•¥ prefill tokenï¼ŒæŠŠ `output_len` å›ºå®šä¸º `input_len`ï¼Œæ–¹ä¾¿æ¨ªå‘å¯¹æ¯” decode æ€§èƒ½ã€‚

é»˜è®¤é…ç½®ï¼ˆä¸åŠ å‚æ•°ç›´æ¥è·‘ï¼‰ï¼š
- `--attn-backend flash triton sdpa.math`
- `--batch-sizes 16 32 64`
- `--input-lens 512 1024 2048`ï¼ˆå¹¶ä¸” `output_len=input_len`ï¼‰

ç¤ºä¾‹ï¼š
```bash
python bench_my.py \
  --attn-backend flash triton sdpa.math \
  --batch-sizes 16 32 64 \
  --input-lens 512 1024 2048 \
  --max-model-len 4096
```

æ³¨æ„äº‹é¡¹ï¼š
- ç”±äºæˆ‘ä»¬æŠŠ `output_len` å›ºå®šä¸º `input_len`ï¼Œæ‰€ä»¥éœ€è¦æ»¡è¶³ `2*max(input_lens) <= max_model_len`ï¼Œå¦åˆ™è„šæœ¬ä¼šç›´æ¥æŠ¥é”™æé†’ä½ è°ƒå‚ã€‚
- è¡¨æ ¼é‡Œæ˜¾ç¤ºçš„ `OOM` è¡¨ç¤ºè¯¥ç»„åˆåœ¨å½“å‰ GPU/æ˜¾å­˜é…ç½®ä¸‹è·‘ä¸åŠ¨ï¼ˆå¸¸è§äºå¤§ batch + é•¿åºåˆ—ï¼‰ã€‚



# bench

## sdpa.math < triton < flash

ç®€å•çš„å¯¹æ¯”ï¼ˆbench.pyï¼‰ï¼š

```
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench.py --attn-backend sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Total: 65536tok, Time: 296.42s, Throughput: 221.09tok/s
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench.py --attn-backend triton
`torch_dtype` is deprecated! Use `dtype` instead!
Total: 65536tok, Time: 38.37s, Throughput: 1708.14tok/s
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench.py --attn-backend flash
`torch_dtype` is deprecated! Use `dtype` instead!
Total: 65536tok, Time: 16.04s, Throughput: 4085.22tok/s
```

æ›´åŠ å…¨é¢çš„å¯¹æ¯”ï¼ˆbench_my.py)ï¼š

```
================================================================================
CROSSOVER ANALYSIS (tok/s, output_len=input_len)
================================================================================
Batch Size 1   |        flash |       triton |    sdpa.math
-----------------------------------------------------------
       512 |          402 |           61 |           18
      1024 |          397 |           32 |           19
      2048 |          369 |           16 |           18

Batch Size 2   |        flash |       triton |    sdpa.math
-----------------------------------------------------------
       512 |          716 |          116 |           34
      1024 |          687 |           62 |           34
      2048 |          609 |           33 |           34

Batch Size 4   |        flash |       triton |    sdpa.math
-----------------------------------------------------------
       512 |         1305 |          231 |           69
      1024 |         1215 |          125 |           69
      2048 |          967 |           65 |           69

Batch Size 8   |        flash |       triton |    sdpa.math
-----------------------------------------------------------
       512 |         2377 |          460 |          139
      1024 |         1925 |          247 |          141
      2048 |         1385 |          129 |          108

Batch Size 16  |        flash |       triton |    sdpa.math
-----------------------------------------------------------
       512 |         3574 |          885 |          279
      1024 |         2670 |          482 |          209
      2048 |         1715 |          252 |          113

Batch Size 32  |        flash |       triton |    sdpa.math
-----------------------------------------------------------
       512 |         5318 |         1719 |          388
      1024 |         3541 |          935 |          217
      2048 |         2132 |          489 |          OOM

Batch Size 64  |        flash |       triton |    sdpa.math
-----------------------------------------------------------
       512 |         6583 |         3089 |          OOM
      1024 |         4076 |         1693 |          OOM
      2048 |         2099 |          489 |          OOM
```

è¿™é‡Œä¹Ÿæœ‰ä¸€å¼ bszå¢å¤§ï¼ŒGPUåˆ©ç”¨ç‡çˆ¬å‡çš„å›¾ï¼Œæ…¢æ…¢å˜æˆcompute-bound

![](./img/é€æ¸æ‰“æ»¡çš„å›¾.png)

## sdpa vs flashï¼ˆæ²¡æœ‰å¯æ¯”æ€§ï¼‰

```
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench_my.py --attn-backend flash sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
^[[A^[[B
================================================================================
CROSSOVER ANALYSIS
================================================================================
 Input Len |  Flash (ms) |    Flash tp |  SDPA (ms) |    SDPA tp |   Winner |  Speedup
------------------------------------------------------------------------------------------------
       512 |    3075.008 |        5328 |  42203.928 |        388 |    Flash |   13.72x
      1024 |    4267.898 |        3839 |  64877.147 |        253 |    Flash |   15.20x
      1536 |    5468.262 |        2996 |  88115.733 |        186 |    Flash |   16.11x
      2048 |    6651.592 |        2463 | 111999.628 |        146 |    Flash |   16.84x
      2560 |    7869.468 |        2082 | 136555.871 |        120 |    Flash |   17.35x
      3072 |    9091.268 |        1802 | 161280.169 |        102 |    Flash |   17.74x
      3584 |   10328.921 |        1586 | 186514.650 |         88 |    Flash |   18.06x
```

# ä¸ºä»€ä¹ˆå¿…é¡»åštriton paged attnï¼Ÿ

## åŸæ¥çš„åšæ³•ï¼šSDPAè®¾è®¡è¦æ±‚ç»„è¿ç»­çš„å¤§bufferï¼Œå¤±å»äº†paged-attnçš„æ„ä¹‰ï¼Œå¿…ç„¶çˆ†

Eageræ²¡æœ‰OOMçš„åŸå› æ˜¯ï¼šåœ¨attention_sdpa.pyä»£ç é‡Œï¼Œdecodeåˆ†æ”¯ä¸”ä¼ å…¥block-tableçš„æƒ…å†µä¸‹ï¼Œä¼šå¯¹æ‰€æœ‰kvcacheç»„ä¸€ä¸ªdense bufferï¼Œmax_seq_len = max_blocks * block_sizeï¼Œç„¶åç”¨ flat_idx = block_ids * block_size + offset å»ä»æ•´å— KV cache é‡Œ gather å‡ºä¸€ä¸ª [B, S, num_kv_heads, head_dim] çš„ dense bufferã€‚è¿™ä¸€ç‚¹å¾ˆåƒæ˜¾å­˜ï¼Œæ‰€ä»¥ä¼šOOMå´©æ‰ã€‚bench_my.pyé‡Œdecodeé•¿åº¦åªæœ‰512ï¼Œä¸ä¼šOOMï¼›è€Œbench.pyé‡Œï¼Œdecodeé•¿åº¦æ˜¯1024ï¼Œä¼šOOMã€‚

æ²¡æœ‰OOMæ˜¯å»ºç«‹åœ¨ start with "sdpa" enforce-eagerçš„æƒ…å†µä¸‹çš„ã€‚åªè¦åˆ æ‰äº†è¿™ä¸ªè®¾å®šï¼Œè®©sdpa.mathåœ¨decodeæ—¶å€™èµ°cudagraphï¼Œä¸ç®¡configè°ƒæ•´å¤šå°éƒ½æŠ¥é”™OOMã€‚ä¿ç•™sdpa.mathä½œä¸ºbaselineåˆ†æ”¯ã€‚

sdpa.mathä½¿ç”¨eagerï¼Œç”¨bench.pyåœ¨bsz=64ï¼Œinput=1024ï¼Œoutput=512æƒ…å†µä¸‹è·‘å‡ºæ¥ï¼š
```
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench.py --attn-backend sdpa.math
Total: 32768tok, Time: 127.40s, Throughput: 257.21tok/s
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench.py --attn-backend flash
Total: 32768tok, Time: 7.33s, Throughput: 4468.50tok/s
```

## ç°åœ¨çš„åšæ³•ï¼šPrefill é˜¶æ®µï¼šä½¿ç”¨è‡ªå®šä¹‰ Triton FlashAttentionï¼ˆé€‚åˆè¿ç»­ bufferï¼‰ï¼›Decode é˜¶æ®µï¼šä½¿ç”¨ Triton å®ç°çš„ Paged-Attentionï¼ˆé¿å… SDPA çš„å†…å­˜è¿ç»­åŒ–é—®é¢˜ï¼‰

| é˜¶æ®µ   | åšæ³• | å…³é”®ç‚¹ |
| --- | --- | --- |
| Prefill | è‡ªå®šä¹‰ Triton FlashAttentionï¼ˆvarlenï¼‰ | è¿ç»­ bufferï¼Œæ–¹ä¾¿æ•è·/å¤ç”¨ï¼Œé¿å… SDPA çš„ graph é™åˆ¶ |
| Decode | Triton Paged-Attention | ç›´æ¥ç”¨ block-table å®šä½ç‰©ç†å—ï¼Œä¸åš dense bufferï¼Œæ˜¾å­˜å¼€é”€éš BLOCK_N è€Œéå…¨é•¿ |
| SDPAï¼ˆbaselineï¼‰ | éœ€è¦æŠŠ KV è¿ç»­åŒ–åå†å–‚ SDPA | ä¼šç»„ [B, S, H, D] å¤§ bufferï¼Œé•¿åºåˆ— OOMï¼Œä¸” Graph æ•è·ä¸‹æ˜“ç‚¸ |

**æ ¸å¿ƒä»£ç ç‰‡æ®µï¼špaged-attn å¦‚ä½•ä» block-table æ‹¿ KVï¼ˆæ‘˜è‡ª `nanovllm/layers/attention_triton.py`ï¼‰**

```python
# å…³é”®ï¼šæŒ‰ chunk å¤„ç†ï¼Œæ¯æ¬¡åªçœ‹ BLOCK_N ä¸ª token
max_chunks = tl.cdiv(max_num_blocks * block_size, BLOCK_N)
for chunk_idx in range(max_chunks):
    token_start = chunk_idx * BLOCK_N
    if token_start < context_len:
        offs_n = token_start + tl.arange(0, BLOCK_N)
        mask_n = offs_n < context_len

        # è®¡ç®—é€»è¾‘ block å·å’Œå—å†…åç§»
        block_num = offs_n // block_size
        block_offset = offs_n % block_size

        # block-table â†’ ç‰©ç†å— id
        block_table_offset = batch_idx * max_num_blocks + block_num
        physical_block_idx = tl.load(block_tables_ptr + block_table_offset)

        # ç›´æ¥ä»åˆ†é¡µ KV cache è¯» K/Vï¼ˆæ— éœ€è¿ç»­åŒ–ï¼‰
        k_offset = (
            physical_block_idx * block_size * num_kv_heads * head_dim
            + block_offset * num_kv_heads * head_dim
            + kv_head_idx * head_dim
            + offs_d
        )
        k_vec = tl.load(k_cache_ptr + k_offset)   # K
        # ... è®¡ç®— qk / online-softmax ...
        v_offset = (...)  # åŒä¸Šæ‹¼å‡º V çš„ç‰©ç†åœ°å€
        v_vec = tl.load(v_cache_ptr + v_offset)   # V
        # acc = acc + weight * v_vec
```

è¦ç‚¹ï¼šä¸ç»„å¤§ bufferï¼›é€»è¾‘ç´¢å¼•é€šè¿‡ block-table æ˜ å°„åˆ°ç‰©ç† KV blockï¼ŒæŒ‰ chunk åšåœ¨çº¿ softmaxï¼Œæ˜¾å­˜åªä¸ BLOCK_N æˆæ­£æ¯”ã€‚

| å†…å­˜/è®¿é—® | SDPAï¼ˆdense bufferï¼‰ | Triton paged-attn |
| --- | --- | --- |
| å•æ¬¡ä¸´æ—¶åˆ†é… | `2 Ã— B Ã— S Ã— H Ã— D`ï¼ˆä¼šéš max_seq_len çˆ†ç‚¸ï¼ŒOOM æ ¹æºï¼‰ | `BLOCK_N Ã— H Ã— D`ï¼ˆä¸åºåˆ—æ€»é•¿æ— å…³ï¼‰ |
| è®¿é—®æ¨¡å¼ | å…ˆ gather å‡ºè¿ç»­ [B, S, H, D] å†è®¡ç®— | ç›´æ¥ç”¨ block-table å®šä½ç‰©ç†å—ï¼Œè¾¹å–è¾¹ç®—ï¼Œæ—  gather |

å†…å­˜ä½¿ç”¨å¯¹æ¯”ï¼ˆå‡è®¾ Batch=32, Max_seq_len=4096, Heads=32, Head_dim=128ï¼Œfp16ï¼‰ï¼š

| å®ç°  | å†…å­˜æ¶ˆè€—                                   | OOM é£é™© |
| --- | -------------------------------------- | --- |
| SDPA | `2 Ã— 32 Ã— 4096 Ã— 32 Ã— 128 Ã— 2 bytes â‰ˆ 2GB` | âŒ æé«˜ |
| Triton | `2 Ã— BLOCK_N Ã— 32 Ã— 128 Ã— 2 bytes`ï¼ˆBLOCK_N=64 â†’ â‰ˆ16KBï¼‰ | âœ… æä½ |


# Future Plan(nano-moe coming soon)

å› ä¸ºä¸€ç›´åœ¨ç ”ç©¶moeæ¨ç†ä¼˜åŒ–ï¼Œæ‰€ä»¥æƒ³åœ¨nanovllmä¸Šå®ç°ä¸‹é¢è¿™å‡ ä¸ªç‰¹æ€§ï¼ŒæŠŠè¿™ä¸ªä»“åº“æ…¢æ…¢è½¬å˜æˆ`nano-moe`å“ˆå“ˆå“ˆğŸ˜„ï¼š

-[ ] æ”¯æŒdpsk-moe

  - [ ] Nano vllm triton mla

  - [ ] Nano vllm triton moe kernel fusion  https://zhuanlan.zhihu.com/p/21251657579

- [x] Nano vllm triton paged-attn

- [ ] Nano vllm eplb

- [ ] Nano vllm shared-expert-overlap

# å¼€å‘è®°å½•ï¼ˆå­˜æ¡£ï¼‰ï¼šSDPA æ›¿ä»£ flash-attn çš„è¸©å‘
* ä¸»è¦å·¥ä½œé‡ï¼šä» block-tables æ‰¾åˆ°å¯¹åº” kvcacheï¼Œå†å–‚ç»™ SDPAã€‚
* ä¸ CUDA graph çš„å…¼å®¹é—®é¢˜ï¼ˆcommit: https://github.com/ztxdcyy/nano-vllm-learn/commit/2f1a0ae2df9f7729494c5c70caf010dd786d2b5eï¼‰ï¼š
  1) æ•è· graph ç¦æ­¢ host æ“ä½œï¼ŒåŸæœ¬çš„ tolist/item éœ€ç§»é™¤ã€‚
  2) æ•è· graph çš„ dummy input ä¼šæœ‰ `context_lens=torch.zeros(...)`ï¼Œè€æ–­è¨€ `max_seq_len>0` ä¼šå†²çªï¼Œæ”¹æˆæ›´é²æ£’çš„åˆ¤æ–­ã€‚
* åç«¯ä¼ é€’è¸©å‘ï¼šæ¨¡å‹åªè¯» Qwen3Configï¼ˆhf_configï¼‰ï¼Œä¸è¯»è¿è¡Œæ—¶ Configã€‚å¿…é¡»æŠŠè‡ªå®šä¹‰ `attn_backend` å†™è¿› hf_configï¼Œå¦åˆ™å§‹ç»ˆè·‘ flash-attnï¼ˆattn_sdpa.py ä¸ä¼šè¢«ç”¨åˆ°ï¼‰ã€‚

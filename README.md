# nano-vllm-learn
# ç‰¹ç‚¹
1. è¯¦ç»†çš„æ³¨é‡Šï¼Œæ–¹ä¾¿å­¦ä¹ 
2. without-flash-attnçš„å®ç°ï¼šæˆ‘è§‰å¾—flash-attnåœ¨æœ€å¼€å§‹å®‰è£…ç¯å¢ƒæ—¶å€™æ—¶å€™éœ€è¦ç¼–è¯‘æ¯”è¾ƒèŠ±æ—¶é—´ï¼Œå°±æƒ³åšä¸€ä¸ªwo-flash-attnçš„ç‰ˆæœ¬ã€‚mainåˆ†æ”¯å·²ç»åˆå¹¶äº†å®Œæ•´å®ç°ï¼Œå¹¶ä¸”æ”¹äº†benchï¼Œä¸‹é¢è¯´ï¼›wo_flash_attnåˆ†æ”¯è®°å½•äº†å¼€å‘è¿‡ç¨‹ã€‚


# é‡‡ç”¨SDPAä»£æ›¿flash-attnï¼š
* ä¸»è¦å·¥ä½œé‡æ˜¯å¦‚ä½•ä»block-tablesé‡Œå–åˆ°å¯¹åº”çš„kvcacheã€‚
* ä¸»è¦è¸©å‘çš„åœ°æ–¹æ˜¯å’Œcudagraphçš„å…¼å®¹é—®é¢˜ï¼šå¯ä»¥çœ‹è¿™ä¸ª[commit](https://github.com/ztxdcyy/nano-vllm-learn/commit/2f1a0ae2df9f7729494c5c70caf010dd786d2b5e)
1. åœ¨æ•æ‰cudagraphçš„æ—¶å€™ç¦æ­¢ host ä¾§çš„æ“ä½œï¼Œå…·ä½“å¯ä»¥çœ‹kaichaoè¿™ç¯‡æ–‡ç« ï¼Œæ‰€ä»¥åŸæ¥çš„tolistã€itemè¿™äº›éƒ½ä¸èƒ½ç”¨
2. è¿˜æ˜¯æ•æ‰cudagraphçš„æ—¶å€™ï¼ˆModelRunner.capture_cudagraphï¼‰ï¼Œdummyinputåœ¨æ„é€ çš„æ—¶å€™ï¼Œcontext_lens=torch.zeros(...)ã€‚å’Œæˆ‘åŸæ¥çš„ä¸€ä¸ªassertå†²çªäº†`assert isinstance(max_seq_len, int) and max_seq_len > 0, "max_seq_len å¿…é¡»æ˜¯æ­£æ•´æ•°"`ï¼Œæ·»åŠ äº†æ›´é²æ£’çš„åˆ¤æ–­æ¥å…¼å®¹capture cudagraphè·‘dummyinputçš„åœºæ™¯ã€‚
* è¸©å‘çš„åœ°æ–¹+1ï¼šä¼ é€’backendçš„æ—¶å€™ä¸€ç›´æ²¡ä¼ hf_configï¼Œå¯¼è‡´ä¸€ç›´è·‘çš„éƒ½æ˜¯flash_attnä¹Ÿå°±æ˜¯attn_sdpa.pyå®Œå…¨æ²¡è¢«ç”¨ä¸Šï¼æ‰€ä»¥åŸºæœ¬ä¸Šæµ‹å‡ºæ¥ååæ²¡å˜ï¼ï¼ï¼ï¼ˆğŸ¥²å°´å°¬â€¦â€¦ï¼‰

åœ¨è¿™å¥—ä»£ç é‡Œï¼Œæ¨¡å‹æ„é€ ç”¨çš„æ˜¯è¯»çš„æ˜¯ Qwen3Config å®ä¾‹ï¼ˆdataclassï¼šconfig.hf_configï¼‰ï¼ŒQwen3DecoderLayer/Qwen3Attention åªçœ‹å®ƒï¼šattn_backend=getattr(config, "attn_backend", "flash")ã€‚LLMEngine.Config æ˜¯è¿è¡Œæ—¶åŒ…è£…ï¼Œæ¨¡å‹åªä¼šæ‹¿åˆ°hf_configï¼ˆQwen3Configï¼‰ä¸ä¼šæŠŠ å¤§çš„è¿è¡Œæ—¶Config ä¼ è¿›å»ã€‚æ‰€ä»¥ä¸æŠŠ attn_backend å†™å› hf_configï¼Œæ¨¡å‹ä¾§æ°¸è¿œæ‹¿ä¸åˆ°ä½ ä¼ çš„åç«¯ï¼Œé»˜è®¤ä¸º flashã€‚ä¼ é€’è¿‡å»åªæ˜¯ä¸ºäº†è®© HF config æºå¸¦è¿™ä¸ªè‡ªå®šä¹‰å­—æ®µï¼Œä½¿æ¨¡å‹èƒ½è¯»åˆ°ã€‚


# bench

```
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench_my.py --attn-backend flash sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
^[[A^[[B
================================================================================
CROSSOVER ANALYSIS
================================================================================
 Input Len |  Flash (ms) |    Flash tp |  SDPA (ms) |    SDPA tp |   Winner |  Speedup
------------------------------------------------------------------------------------------------
       512 |    3075.008 |        5328 |  42203.928 |        388 |    Flash |   13.72x
      1024 |    4267.898 |        3839 |  64877.147 |        253 |    Flash |   15.20x
      1536 |    5468.262 |        2996 |  88115.733 |        186 |    Flash |   16.11x
      2048 |    6651.592 |        2463 | 111999.628 |        146 |    Flash |   16.84x
      2560 |    7869.468 |        2082 | 136555.871 |        120 |    Flash |   17.35x
      3072 |    9091.268 |        1802 | 161280.169 |        102 |    Flash |   17.74x
      3584 |   10328.921 |        1586 | 186514.650 |         88 |    Flash |   18.06x
```

å±…ç„¶æ²¡æœ‰OOMï¼Ÿä¸ä¼šå§ã€‚æˆ‘ä»¬åœ¨bench.pyé‡Œä¸ºä»€ä¹ˆOOMäº†ï¼Ÿ

## åˆ é™¤official benchçš„éšæœºæ€§ï¼Œæ·»åŠ å‘½ä»¤è¡Œå‚æ•°`--attn-backend`
æˆ‘ä¿®æ”¹äº†[official benchä»£ç ](bench.py)çš„éšæœºæ€§ï¼ŒåŸæ¥ä»–çš„ä»£ç æ˜¯å®šä¸‹ä¸€ä¸ªmax-input-lenå’Œmax-output-lenï¼Œç„¶å`randint(100, max)`éšæœºå–è¾“å…¥è¾“å‡ºé•¿åº¦ï¼Œæˆ‘è§‰å¾—è¿˜æ˜¯å®šä¸‹æ¥æˆ‘æ¯”è¾ƒå®‰å¿ƒï¼Œå°±æŠŠrandintåˆ æ‰äº†ã€‚

åŒæ—¶æ–°å¢äº†`attn-backend`çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨æ¥æŒ‡å®šä½¿ç”¨sdpaè¿˜æ˜¯flash-attnï¼Œé»˜è®¤æ˜¯flashã€‚

è·‘ä¸‹æ¥ç»“æœï¼š
```
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench.py --attn-backend flash
`torch_dtype` is deprecated! Use `dtype` instead!
Total: 262144tok, Time: 64.33s, Throughput: 4075.22tok/s
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench.py --attn-backend sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Total: 262144tok, Time: 64.34s, Throughput: 4074.61tok/s
```

## æ–°å¢äº†è¡¨æ ¼å¯¹æ¯”
ä¸ºäº†å¯¹æ¯”æˆ‘çš„sdpa backendå’Œflash-attn backendæ€§èƒ½å·®äº†å¤šå°‘æˆ‘æ–°å¢äº†ä¸€ä¸ª[`bench_my.py`](bench_my.py)ï¼Œå®ƒèƒ½ç”Ÿæˆä¸€ä¸ªè¡¨æ ¼ï¼Œå¯¹æ¯”latencyå’Œthroughputä»¥åŠåŠ é€Ÿå€æ•°ï¼Œå‚è€ƒçš„æ˜¯[here](https://github.com/Wenyueh/MinivLLM/blob/main/benchmark_decoding.py)

```
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench.py --model-path /root/autodl-tmp/models/Qwen3-0.6B   --attn-backend flash sdpa  --num-seqs 256 --input-lens 1024   --output-len 1024
`torch_dtype` is deprecated! Use `dtype` instead!

================================================================================
CROSSOVER ANALYSIS
================================================================================
 Input Len |  Flash (ms) |    Flash tp |  SDPA (ms) |    SDPA tp |   Winner |  Speedup
------------------------------------------------------------------------------------------------
      1024 |   64340.523 |        4074 |  64022.840 |       4095 |     SDPA |    1.00x
```
å½“ç„¶ä¹Ÿå¯ä»¥ä¼ å…¥ä¸€ä¸ªlistï¼Œä¼šç”ŸæˆçœŸæ­£çš„è¡¨æ ¼ã€æœ‰ä¸ªå°é™åˆ¶ï¼Œinput+outputä¸è¦è¶…è¿‡max-model-lenï¼Œå¦åˆ™æ¨¡å‹éƒ½æ˜¯èƒ¡è¨€ä¹±è¯­ã€‘ï¼š
```
(nano_venv) root@autodl-container-b95c4d8452-4b3d06c8:~/workspace/nano-vllm-learn# python bench_my.py 
`torch_dtype` is deprecated! Use `dtype` instead!

================================================================================
CROSSOVER ANALYSIS
================================================================================
 Input Len |  Flash (ms) |    Flash tp |  SDPA (ms) |    SDPA tp |   Winner |  Speedup
------------------------------------------------------------------------------------------------
       512 |    3093.975 |        5295 |   3085.169 |       5311 |     SDPA |    1.00x
      1024 |    4248.941 |        3856 |   4244.249 |       3860 |     SDPA |    1.00x
      1536 |    5438.311 |        3013 |   5442.521 |       3010 |    Flash |    1.00x
      2048 |    6606.076 |        2480 |   6604.461 |       2481 |     SDPA |    1.00x
      2560 |    7773.946 |        2108 |   7800.910 |       2100 |    Flash |    1.00x
      3072 |    8971.982 |        1826 |   9001.928 |       1820 |    Flash |    1.00x
      3584 |   10226.610 |        1602 |  10228.934 |       1602 |    Flash |    1.00x
```

## åˆ†æ
å¯ä»¥çœ‹åˆ°SDPAå’Œflash-attnåœ¨ç«¯åˆ°ç«¯çš„æƒ…å†µä¸‹åŸºæœ¬æ²¡æœ‰å·®è·ï¼Œsdpaæœ¬è´¨ä¹Ÿæ˜¯é‡‡ç”¨flash-attnæ€æƒ³ä¼˜åŒ–çš„kernelã€‚
Attention æœºåˆ¶å“ªå®¶å¼ºï¼ŸSDPAã€FlashAttentionã€xFormersã€æ‰‹åŠ¨å®ç°å…¨é¢å¯¹æ¯” - ä¸€æ¡æ”¾æµªä¸ç¾çš„çˆ¬è™«çš„æ–‡ç«  - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/1898470649938293363


æˆ‘è¿˜éœ€è¦å‹æµ‹ä¸€ä¸‹ï¼Œçœ‹çœ‹SDPAä»€ä¹ˆæ—¶å€™ä¼šOOMã€‚æˆ‘è·‘äº†[å¤§ä½¬çš„ä»£ç ](https://github.com/Wenyueh/MinivLLM/blob/main/benchmark_prefilling.py)å‘ç°å¯¹æ¯”tritonå®ç°ï¼Œfaè¿˜æ˜¯åœ¨æ˜¾å­˜ä¸Šæœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œåœ¨é•¿åºåˆ—åœºæ™¯ä¸‹ï¼Œåªæœ‰flashä¾æ—§æ´»ç€ï¼ï¼ï¼

```
================================================================================
CROSSOVER ANALYSIS
================================================================================
   Seq Len |   Naive (ms) |   Flash (ms) |     Winner |    Speedup
--------------------------------------------------------------------------------
        16 |        0.029 |        0.053 |      Naive |      1.80x
        32 |        0.028 |        0.056 |      Naive |      1.97x
        48 |        0.028 |        0.098 |      Naive |      3.44x
        64 |        0.030 |        0.059 |      Naive |      1.97x
        80 |          OOM |        0.055 |      Flash |        N/A
        96 |          OOM |        0.053 |      Flash |        N/A
       112 |          OOM |        0.053 |      Flash |        N/A
       128 |          OOM |        0.054 |      Flash |        N/A
       192 |          OOM |        0.062 |      Flash |        N/A
       256 |          OOM |        0.070 |      Flash |        N/A
       512 |          OOM |        0.134 |      Flash |        N/A
      1024 |          OOM |        0.343 |      Flash |        N/A

================================================================================
KERNEL LAUNCH ANALYSIS
================================================================================

For 2 sequences Ã— 60 tokens:
  Naive Triton grid:    (2, 32)
  Naive total kernels:  64

  Flash Attention grid: (2, 32, 2)
  Flash total kernels:  128

  Ratio: Flash launches 2.0x more kernels

  Each kernel launch has ~5-20Î¼s overhead
  Extra overhead: ~640Î¼s = 0.64ms
```

# Future Plan(nano-moe coming soon)

å› ä¸ºä¸€ç›´åœ¨ç ”ç©¶moeæ¨ç†ä¼˜åŒ–ï¼Œæ‰€ä»¥æƒ³åœ¨nanovllmä¸Šå®ç°ä¸‹é¢è¿™å‡ ä¸ªç‰¹æ€§ï¼ŒæŠŠè¿™ä¸ªä»“åº“æ…¢æ…¢è½¬å˜æˆ`nano-moe`å“ˆå“ˆå“ˆğŸ˜„ï¼š

-[ ] æ”¯æŒdpsk-moe

  - [ ] Nano vllm triton mla

  - [ ] Nano vllm triton moe kernel fusion  https://zhuanlan.zhihu.com/p/21251657579

- [x] Nano vllm triton paged-attn

- [ ] Nano vllm eplb

- [ ] Nano vllm shared-expert-overlap





